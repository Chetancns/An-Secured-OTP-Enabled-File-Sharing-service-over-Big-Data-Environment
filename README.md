# An-Secured-OTP-Enabled-File-Sharing-service-over-Big-Data-Environment
The high volume, velocity, and variety of data being produced by diverse scientific and
business domains challenge standard solutions of data management, requiring them to
scale while ensuring security and dependability. A fundamental problem is where and
how to store the vast amount of data that is being continuously generated. Private
infrastructures are the first option for many organizations. However, creating and
maintaining data centers is expensive, requires specialized workforce, and can create
hurdles to sharing. Conversely, attributes like cost-effectiveness, ease of use, and
(almost) infinite scalability make public cloud services natural candidates to address
data storage problems.


  File sharing has been an essential part of this century. Using various applications,
files can be shared to large number of users. For the purpose of storage, the Hadoop
Distributed File System (HDFS) can be used. HDFS is mainly used for the unstructured
data analysis. The HDFS handles large size of files in a single server. Common sharing
methods like removable media, servers or computer network, World Wide Web based
hyperlink documents. In the proposed project, the files are merged using MapReduce
programming model on Hadoop. This process improves the performance of Hadoop by
rejecting the files which are larger than the size of Hadoop and reduces the memory size
required by the NameNode
